# Copyright 2026 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law of a greedor agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Reusable workflow to run benchmarks defined in a benchmark registry.
#
# This workflow generates and executes a parallel matrix of benchmark jobs
# based on the provided registry_file and workflow_type.
#
# Note that GITHUB_WORKSPACE env var is used instead of github.workspace
# context reference due to https://github.com/actions/runner/issues/2058. 
# Specifically, github.workspace and runner.workspace don't point to container 
# valid paths when executing inside a container job.
name: Run benchmarks

on:
  workflow_call:
    inputs:
      registry_file:
        description: "Path to the .pbtxt benchmark registry file, relative to the repository root."
        required: true
        type: string
      workflow_type:
        description: "The workflow type to run (e.g., PRESUBMIT)."
        required: true
        type: string
      ml_actions_ref:
        description: >
          The branch, tag, or SHA of google-ml-infra/actions to use. Defaults to 'main'.
          Note: For runs triggered from within the google-ml-infra/actions repo
          (e.g., a PR), the commit SHA (github.sha) is used automatically to test changes.
        required: false
        type: string
        default: 'main'
      publish_metrics:
        description: "If true, publishes benchmark results to Google Cloud Pub/Sub."
        required: false
        type: boolean
        default: false
      pub_sub_gcp_project_id:
        description: "Google Cloud Project ID for Pub/Sub."
        required: false
        type: string
        default: "ml-oss-benchmarking-production"
      pub_sub_gcp_topic_id:
        description: "Pub/Sub Topic ID to publish results to."
        required: false
        type: string
        default: "public-results-prod"
      ab_mode:
        description: "If true, runs A/B comparison (baseline vs experiment) and generates an A/B report."
        required: false
        type: boolean
        default: false
      experiment_ref:
        description: "Git ref for the experiment. Defaults to current commit SHA."
        required: false
        type: string
        default: ""
      baseline_ref:
        description: "Git ref for the baseline. Defaults to PR base or main."
        required: false
        type: string
        default: ""
      post_pr_comment:
        description: "If true (and ab_mode is enabled), posts the A/B report as a sticky comment on the PR."
        required: false
        type: boolean
        default: true
      job_id:
        description: "A unique identifier for the top-level job (e.g. 'e2e-test'). Used to namespace artifacts. If empty, a random ID is generated."
        required: false
        type: string
        default: ""

permissions:
  contents: read  # Required for actions/checkout.
  pull-requests: write  # Required for posting PR comments.

jobs:
  generate_matrix:
    name: Generate matrix
    runs-on: linux-x86-n2-16
    container:
      image: us-docker.pkg.dev/ml-oss-artifacts-published/ml-public-container/ml-build:latest@sha256:fa95dacafd53199ab68e3c28c488c0cc0d5fad3a4b0ac20af27f2bd0a061a55d  # ratchet:us-docker.pkg.dev/ml-oss-artifacts-published/ml-public-container/ml-build:latest
    outputs:
      matrix: ${{ steps.generate.outputs.matrix }}
      ml_actions_ref: ${{ steps.resolve_refs.outputs.ml_actions_ref }}
      job_id: ${{ steps.gen_job_id.outputs.job_id }}
    steps:
      - name: Check out user repo
        uses: actions/checkout@93cb6efe18208431cddfb8368fd83d5badbf9bfd  # ratchet:actions/checkout@v5
        with:
          repository: ${{ github.repository }}
          ref: ${{ github.sha }}
          path: user_repo
          persist-credentials: false

      - name: Generate Job ID
        id: gen_job_id
        shell: bash
        env:
          INPUT_JOB_ID: ${{ inputs.job_id }}
        run: |
          # If user provided a job_id, use it. Otherwise, generate one.
          if [[ -n "$INPUT_JOB_ID" ]]; then
            echo "job_id=$INPUT_JOB_ID" >> "$GITHUB_OUTPUT"
          else
            # Generates 16 bytes of hex (32 chars), equivalent to a UUID length.
            JOB_ID="$(openssl rand -hex 16)"
            echo "job_id=$JOB_ID" >> "$GITHUB_OUTPUT"
          fi

      - name: Resolve Git refs
        id: resolve_refs
        shell: bash
        env:
          INPUT_BASELINE: ${{ inputs.baseline_ref }}
          INPUT_EXPERIMENT: ${{ inputs.experiment_ref }}
          IS_PR: ${{ github.event_name == 'pull_request' }}
          PR_BASE: ${{ github.base_ref }}
          DEFAULT_BRANCH: ${{ github.event.repository.default_branch }}
          # If we are in the actions repo itself, use the current SHA. Otherwise, use the input ref.
          ML_ACTIONS_REF: ${{ github.repository == 'google-ml-infra/actions' && github.sha || inputs.ml_actions_ref }}
          SHA: ${{ github.sha }}
        run: |
          # Resolve ML Actions ref
          echo "ml_actions_ref=$ML_ACTIONS_REF" >> "$GITHUB_OUTPUT"

          # Resolve experiment ref.
          # Default to current SHA if not provided.
          if [[ -z "$INPUT_EXPERIMENT" ]]; then
            EXPERIMENT_REF="$SHA"
          else
            EXPERIMENT_REF="$INPUT_EXPERIMENT"
          fi

          # Resolve baseline ref.
          # Priority order:
          # 1. Manual input (if provided by user)
          # 2. PR base (if triggered by a PR)
          # 3. Default branch (fallback, usually 'main')
          if [[ -n "$INPUT_BASELINE" ]]; then
             BASELINE_REF="$INPUT_BASELINE"
          elif [[ "$IS_PR" == "true" ]]; then
             BASELINE_REF="$PR_BASE"
          else
             BASELINE_REF="$DEFAULT_BRANCH"
          fi

          echo "Resolved git refs: baseline=$BASELINE_REF, experiment=$EXPERIMENT_REF"
          echo "baseline_ref=$BASELINE_REF" >> "$GITHUB_OUTPUT"
          echo "experiment_ref=$EXPERIMENT_REF" >> "$GITHUB_OUTPUT"

      - name: Check out ML actions repo
        uses: actions/checkout@93cb6efe18208431cddfb8368fd83d5badbf9bfd  # ratchet:actions/checkout@v5
        with:
          repository: 'google-ml-infra/actions'
          ref: ${{ steps.resolve_refs.outputs.ml_actions_ref }}
          path: ml_actions
          persist-credentials: false

      - name: Generate benchmark matrix
        env:
          REGISTRY_FILE: ${{ inputs.registry_file }}
          WORKFLOW_TYPE: ${{ inputs.workflow_type }}
          AB_MODE: ${{ inputs.ab_mode }}
          BASELINE_REF: ${{ steps.resolve_refs.outputs.baseline_ref }}
          EXPERIMENT_REF: ${{ steps.resolve_refs.outputs.experiment_ref }}
        id: generate
        shell: bash
        working-directory: ml_actions
        run: |
          set -euo pipefail
          REGISTRY_PATH="$GITHUB_WORKSPACE/user_repo/$REGISTRY_FILE"
          MATRIX_JSON_PATH="$GITHUB_WORKSPACE/matrix.json"

          MATRIX_JSON="$(bazel run //benchmarking/gh_matrix_generator -- \
          --registry_file="$REGISTRY_PATH" \
          --workflow_type="$WORKFLOW_TYPE" \
          --ab_mode="$AB_MODE" \
          --baseline_ref="$BASELINE_REF" \
          --experiment_ref="$EXPERIMENT_REF" | jq -c '.')"

          echo "$MATRIX_JSON" | jq '.' > "$MATRIX_JSON_PATH"
          echo "matrix_json_path=$MATRIX_JSON_PATH" >> "$GITHUB_OUTPUT"
          echo "matrix=$MATRIX_JSON" >> "$GITHUB_OUTPUT"

      - name: Upload matrix JSON artifact
        uses: actions/upload-artifact@ea165f8d65b6e75b540449e92b4886f43607fa02  # ratchet:actions/upload-artifact@v4
        with:
          name: matrix-json-${{ steps.gen_job_id.outputs.job_id }}
          path: ${{ steps.generate.outputs.matrix_json_path }}

  run_benchmarks:
    # For A/B mode, append mode to the job name.
    name: Run benchmark (${{ matrix.config_id }}${{ matrix.ab_test_group && format(' - {0}', matrix.ab_test_group) || '' }})
    needs: generate_matrix
    if: needs.generate_matrix.outputs.matrix != '[]'

    # For A/B mode, conditionally soft fail:
    # If this is a baseline job, continue even if it fails,
    # If this is an experiment job (or standard run), failure is fatal.
    continue-on-error: ${{ matrix.ab_test_group == 'BASELINE' }}

    strategy:
      fail-fast: false
      matrix:
        include: ${{ fromJson(needs.generate_matrix.outputs.matrix) }}

    runs-on: ${{ matrix.runner_label }}
    container:
      image: ${{ matrix.container_image }}

    steps:
      - name: Check out user repo
        uses: actions/checkout@93cb6efe18208431cddfb8368fd83d5badbf9bfd  # ratchet:actions/checkout@v5
        with:
          repository: ${{ github.repository }}
          # For A/B mode, use the checkout ref. Otherwise fall back to SHA.
          ref: ${{ matrix.checkout_ref || github.sha }}
          path: user_repo
          persist-credentials: false

      - name: Get workload commit info
        id: commit_info
        shell: bash
        working-directory: user_repo
        env:
          # For A/B mode, use the checkout ref. Otherwise fall back to standard ref.
          RESOLVED_REF: ${{ matrix.checkout_ref || github.ref_name }}
        run: |
          echo "sha=$(git rev-parse HEAD)" >> "$GITHUB_OUTPUT"
          echo "ref=$RESOLVED_REF" >> "$GITHUB_OUTPUT"

      - name: Check out ML actions repo
        uses: actions/checkout@93cb6efe18208431cddfb8368fd83d5badbf9bfd  # ratchet:actions/checkout@v5
        with:
          repository: 'google-ml-infra/actions'
          ref: ${{ needs.generate_matrix.outputs.ml_actions_ref }}
          path: ml_actions
          persist-credentials: false

      - name: Setup benchmark environment
        shell: bash
        run: |
          # Create TensorBoard output dir and workload artifacts dir.
          TENSORBOARD_OUTPUT_DIR="$GITHUB_WORKSPACE/tblogs"
          WORKLOAD_ARTIFACTS_DIR="$GITHUB_WORKSPACE/artifacts"
          mkdir -p "$TENSORBOARD_OUTPUT_DIR" "$WORKLOAD_ARTIFACTS_DIR"
          
          echo "TENSORBOARD_OUTPUT_DIR: $TENSORBOARD_OUTPUT_DIR"
          echo "WORKLOAD_ARTIFACTS_DIR: $WORKLOAD_ARTIFACTS_DIR"

          # Export created dirs to GITHUB_ENV
          {
            echo "TENSORBOARD_OUTPUT_DIR=$TENSORBOARD_OUTPUT_DIR"
            echo "WORKLOAD_ARTIFACTS_DIR=$WORKLOAD_ARTIFACTS_DIR"
          } >> "$GITHUB_ENV"

      - name: Execute workload
        uses: ./ml_actions/benchmarking/actions/dynamic_action_executor
        with:
          action: ${{ matrix.workload.action }}
          action_inputs: ${{ matrix.workload.action_inputs && toJson(matrix.workload.action_inputs) || '{}' }}

      - name: Upload workload artifacts
        if: always() # Uploads even if the workload fails
        uses: actions/upload-artifact@ea165f8d65b6e75b540449e92b4886f43607fa02  # ratchet:actions/upload-artifact@v4
        with:
          # Format: workload-artifacts-{CONFIG}[-{AB_MODE}]-{JOB_ID}.json
          name: workload-artifacts-${{ matrix.config_id }}${{ matrix.ab_test_group && format('-{0}', matrix.ab_test_group) || '' }}-${{ needs.generate_matrix.outputs.job_id }}
          path: ${{ env.WORKLOAD_ARTIFACTS_DIR }}
          retention-days: 7
          if-no-files-found: ignore

      - name: Parse TensorBoard logs and create benchmark result artifact
        env:
          BENCHMARK_CONFIG_ID: ${{ matrix.config_id }}
          METRIC_SPECS_JSON: '${{ toJson(matrix.metrics) }}'
          WORKFLOW_TYPE: ${{ matrix.workflow_type }}
          RUNNER_LABEL: ${{ matrix.runner_label }}
          RUN_URL: "${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"
          COMMIT_SHA: ${{ steps.commit_info.outputs.sha }}
          BRANCH: ${{ steps.commit_info.outputs.ref }}
          BENCHMARK_RESULTS_DIR: "${{ github.workspace }}/results"
          # Format: benchmark-result-{CONFIG}[-{AB_MODE}]-{JOB_ID}.json
          BENCHMARK_RESULT_FILENAME: >-
            benchmark-result-${{ matrix.config_id }}${{ matrix.ab_test_group && format('-{0}', matrix.ab_test_group) || '' }}-${{ needs.generate_matrix.outputs.job_id }}.json
        id: parse_tb_logs
        working-directory: ml_actions
        shell: bash
        run: |
          set -euo pipefail
          mkdir -p "$BENCHMARK_RESULTS_DIR"          
          TARGET_BENCHMARK_RESULT_FILE_PATH="$BENCHMARK_RESULTS_DIR/$BENCHMARK_RESULT_FILENAME"

          bazel run //benchmarking/tb_parser -- \
            --metric_specs_json="$METRIC_SPECS_JSON" \
            --tblog_dir="$TENSORBOARD_OUTPUT_DIR" \
            --output_dir="$BENCHMARK_RESULTS_DIR" \
            --config_id="$BENCHMARK_CONFIG_ID" \
            --commit_sha="$COMMIT_SHA" \
            --github_run_id="$GITHUB_RUN_ID" \
            --workflow_type="$WORKFLOW_TYPE" \
            --runner_label="$RUNNER_LABEL" \
            --branch="$BRANCH" \
            --run_url="$RUN_URL"

          # Rename the default output to our specific, unique artifact name
          mv "$BENCHMARK_RESULTS_DIR/benchmark_result.json" "$TARGET_BENCHMARK_RESULT_FILE_PATH"

          {
            echo "benchmark_results_dir=$BENCHMARK_RESULTS_DIR"
            echo "benchmark_result_file=$TARGET_BENCHMARK_RESULT_FILE_PATH"
            echo "benchmark_result_artifact_name=${BENCHMARK_RESULT_FILENAME%.json}"
          } >> "$GITHUB_OUTPUT"

      - name: Upload benchmark result
        uses: actions/upload-artifact@ea165f8d65b6e75b540449e92b4886f43607fa02  # ratchet:actions/upload-artifact@v4
        with:
          name: ${{ steps.parse_tb_logs.outputs.benchmark_result_artifact_name }}
          path: ${{ steps.parse_tb_logs.outputs.benchmark_result_file }}

      - name: Run static threshold analyzer
        # For A/B mode, skip static analysis. The ab_analysis job will handle pass/fail logic.
        if: ${{ ! inputs.ab_mode }}
        env:
          METRIC_SPECS_JSON: '${{ toJson(matrix.metrics) }}'
          BENCHMARK_RESULT_FILE: ${{ steps.parse_tb_logs.outputs.benchmark_result_file }}
        id: static_threshold_analyzer
        shell: bash
        working-directory: ml_actions
        run: |
          set -euo pipefail

          bazel run //benchmarking/static_threshold_analyzer -- \
            --metric_specs_json="$METRIC_SPECS_JSON" \
            --benchmark_result_file="$BENCHMARK_RESULT_FILE"

      - name: Publish Results to Pub/Sub
        if: success() && inputs.publish_metrics
        env:
          PROJECT_ID: ${{ inputs.pub_sub_gcp_project_id }}
          TOPIC_ID: ${{ inputs.pub_sub_gcp_topic_id }}
          BENCHMARK_RESULTS_DIR: ${{ steps.parse_tb_logs.outputs.benchmark_results_dir }}
          USER_REPO: ${{ github.repository }}
        shell: bash
        working-directory: ml_actions
        run: |
          set -euo pipefail

          bazel run \
            --noshow_progress \
            //benchmarking/publisher:publish_results -- \
            --project_id="$PROJECT_ID" \
            --topic_id="$TOPIC_ID" \
            --benchmark_results_dir="$BENCHMARK_RESULTS_DIR" \
            --repo_name="$USER_REPO"

  ab_analysis:
    name: A/B Analysis
    needs: [generate_matrix, run_benchmarks]
    runs-on: linux-x86-n2-16
    container:
      image: us-docker.pkg.dev/ml-oss-artifacts-published/ml-public-container/ml-build:latest@sha256:fa95dacafd53199ab68e3c28c488c0cc0d5fad3a4b0ac20af27f2bd0a061a55d  # ratchet:us-docker.pkg.dev/ml-oss-artifacts-published/ml-public-container/ml-build:latest
    if: inputs.ab_mode == true && !cancelled()
    steps:
      - name: Check out ML actions repo
        uses: actions/checkout@93cb6efe18208431cddfb8368fd83d5badbf9bfd  # ratchet:actions/checkout@v5
        with:
          repository: 'google-ml-infra/actions'
          ref: ${{ needs.generate_matrix.outputs.ml_actions_ref }}
          path: ml_actions
          persist-credentials: false

      - name: Download benchmark results
        uses: actions/download-artifact@d3f86a106a0bac45b974a628896c90dbdf5c8093  # ratchet:actions/download-artifact@v4
        with:
          # Only download artifacts created in this specific job
          pattern: benchmark-result-*-${{ needs.generate_matrix.outputs.job_id }}
          path: all_results
          merge-multiple: true

      - name: Run A/B Analyzer
        id: ab_analyzer
        env:
          MATRIX_JSON_STR: ${{ needs.generate_matrix.outputs.matrix }}
          REPO_URL: "${{ github.server_url }}/${{ github.repository }}"
          WORKFLOW_NAME: ${{ github.workflow }}
        shell: bash
        working-directory: ml_actions
        run: |
          set -euo pipefail

          RESULTS_DIR="$GITHUB_WORKSPACE/all_results"
          REPORT_FILE="$GITHUB_WORKSPACE/ab_report.md"

          # Catch non-zero exit code to allow the workflow to proceed to the PR comment step.
          bazel run //benchmarking/ab_analyzer -- \
             --results_dir="$RESULTS_DIR" \
             --matrix_json="$MATRIX_JSON_STR" \
             --output_file="$REPORT_FILE" \
             --repo_url="$REPO_URL" \
             --workflow_name="$WORKFLOW_NAME" || echo "status=FAILURE" >> "$GITHUB_OUTPUT"

          # Set A/B report as job summary
          if [[ -f "$REPORT_FILE" ]]; then
            cat "$REPORT_FILE" >> "$GITHUB_STEP_SUMMARY"
          else
            echo "_No A/B report generated due to analysis failure._" >> "$GITHUB_STEP_SUMMARY"
          fi

      - name: Upload A/B Report Artifact
        uses: actions/upload-artifact@ea165f8d65b6e75b540449e92b4886f43607fa02  # ratchet:actions/upload-artifact@v4
        with:
          name: ab-report-${{ needs.generate_matrix.outputs.job_id }}
          path: ${{ github.workspace }}/ab_report.md

      - name: Post PR Comment
        if: inputs.post_pr_comment == true && github.event_name == 'pull_request' && !cancelled()
        uses: actions/github-script@f28e40c7f34bde8b3046d885e986cb6290c5673b  # ratchet:actions/github-script@v7
        with:
          script: |
            const scriptPath = require('path').resolve('./ml_actions/benchmarking/post_pr_comment/post_pr_comment.js');
            const postComment = require(scriptPath);
            await postComment({ github, context });

      - name: Check Final Status
        shell: bash
        env:
          AB_STATUS: ${{ steps.ab_analyzer.outputs.status }}
        run: |
          if [[ "$AB_STATUS" == "FAILURE" ]]; then
            echo "[FAILURE] A/B Analyzer detected regressions or missing experiments."
            exit 1
          fi
