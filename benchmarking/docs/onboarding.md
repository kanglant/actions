# Onboarding Guide: ML Benchmarking

This guide provides the steps to add a project's benchmarks to the ML Benchmarking Infrastructure. ML Benchmarking is GitHub-native and makes use of GitHub Actions to administer benchmarks.

The system is designed to be agnostic to the benchmark workload, supporting any workload, such as Bazel targets or Python-based scripts.

The system follows two simple contracts:

1. Input: Benchmark registry (e.g. benchmark_registry.pbtxt) defining benchmark requirements.
2. Output: Metric data written via TensorBoard from benchmark scripts.

Our infrastructure handles the following:

- Provisioning the correct GitHub Actions runners.
- Converting defined benchmarks and hardware requirements into GitHub Actions jobs.
- Workload build and dependency installation.
- TensorBoard log parsing and statistic computation.
- Static threshold analysis.

## Step 1: Create a workflow file

First, in your own repository, create a new workflow file in `.github/workflows/` for running benchmarks. 

```yaml
name: Run presubmit benchmarks

on:
  pull_request:
    paths:
      - 'benchmarking/**'

permissions:
  contents: read

jobs:
  run_benchmarks:
    uses: google-ml-infra/actions/.github/workflows/run_benchmarks.yml@<commit | branch | tag>
    with:
      registry_file: "benchmarking/my_registry.pbtxt"
      workflow_type: "PRESUBMIT"
      ml_actions_ref: <commit | branch | tag>
```

### Required permissions

`permissions: contents: read` permission is required in the caller workflow. The reusable workflow's access token inherits permissions from the caller, so the caller must explicitly grant the read rights needed for actions/checkout to succeed.

### ml_actions_ref

You must specify the `ml_actions_ref` input, otherwise it will default to `main`.

This value tells the reusable workflow which version (branch, tag, or SHA) of the google-ml-infra/actions repository to check out for its internal scripts.

For production, use the same stable tag or SHA that's used to pin the reusable workflow file version (e.g. "v1.5.0" for "google-ml-infra/actions/.github/workflows/run_benchmarks.yml@v1.5.0").

### Workflow granularity

We recommend creating a dedicated workflow file for each distinct [workflow_type](https://github.com/google-ml-infra/actions/blob/main/benchmarking/proto/benchmark_registry.proto#L112) 
you plan to support (PRESUBMIT, NIGHTLY, PERIODIC, etc.) to better control scheduling, triggers, and resource allocation.


## Step 2: Create benchmark registry

Next, create a benchmark registry file (.pbtxt). It defines what benchmarks to run and how to run them, based on the [benchmark_registry.proto](https://github.com/google-ml-infra/actions/blob/main/benchmarking/proto/benchmark_registry.proto) schema.

A key part of the registry is defining metrics. You must specify the `metrics.name` field, which must exactly match the tag name used in the TensorBoard logs generated by your benchmark script (covered in the next step). 
Within the metrics block, you specify the statistics (stats) to be calculated (e.g., MEAN, P99) and can optionally configure static threshold analysis using the comparison block.

### Example 1: Bazel workload

```proto
benchmarks {
  name: "my_bazel_benchmark"
  description: "Runs a simple Bazel target."
  owner: "my-team"

  workload {
    bazel_workload {
      execution_target: "//my_project:my_benchmark_binary"
    }
    runtime_flags: "--model_name=resnet"
  }

  hardware_configs {
    hardware_category: CPU_X86
    topology { num_hosts: 1, num_devices_per_host: 1 }
    workflow_type: [PRESUBMIT]

    # Add hardware-specific runtime flags
    runtime_flags: "--precision=fp32"
  }

  update_frequency_policy: QUARTERLY

  metrics {
    # REQUIRED: Must match the TensorBoard tag name (e.g., 'wall_time' in the log)
    name: "wall_time"
    unit: "ms"

    stats {
      stat: MEAN
      comparison: {
        # Configures static threshold analysis against a baseline
        baseline { value: 100.0 } 
        threshold { value: 0.1 }
        improvement_direction: LESS 
      }
    }

    stats {
      stat: P99 
    }
  }
}
```

### Example 2: Python workload

```proto
benchmarks {
  name: "my_python_benchmark"
  description: "Runs a pip-based Python script."
  owner: "my-team"

  workload {
    python_workload {
      script_path: "benchmarking/scripts/run_pallas.py"
      python_version: "3.11"

      # The directory containing your pyproject.toml
      pip_project_path: "."
      
      # Optional dependencies from your pyproject.toml [project.optional-dependencies]
      pip_optional_dependencies: "test"
    }
    runtime_flags: "--model_name=my_kernel"
  }
  
  hardware_configs {
    hardware_category: GPU_L4
    topology { num_hosts: 1, num_devices_per_host: 1 }
    workflow_type: [PRESUBMIT]
    
    # Add hardware-specific optional dependencies
    pip_optional_dependencies: "cuda"
    
    # Add hardware-specific runtime flags
    runtime_flags: "--use_gpu"
  }

  update_frequency_policy: QUARTERLY

  metrics {
    # REQUIRED: Must match the TensorBoard tag name
    name: "throughput"
    unit: "samples/sec"
    
    stats {
      stat: MEAN
      comparison: {
        baseline { value: 5000.0 }
        threshold { value: 0.05 }
        improvement_direction: GREATER
      }
    }

    stats {
      stat: MIN
    }
  }
}
```

## Step 3: Log metrics via TensorBoard

Your benchmark script will need to log metrics via TensorBoard to integrate with the infrastructure. The reusable workflow provides a standard environment variable, `TENSORBOARD_OUTPUT_DIR`, which points to the directory where TensorBoard must write event files.

### API Requirement
You must use a library that writes in the standard TensorBoard format.

- **Required API**: The TensorFlow 2.x Summary API (e.g., tf.summary.create_file_writer).
- **Supported Package**: This API is available in both the full [TensorFlow](https://pypi.org/project/tensorflow) package and the lightweight, standalone [TensorBoard](https://pypi.org/project/tensorboard) package. *Do not use the legacy TensorFlow 1.x tf.summary.FileWriter API, as it is incompatible*.

Example script:

```python
import tensorflow as tf
import os
import sys
import numpy as np

# Get the output directory from the infrastructure.
tblog_dir = os.environ.get("TENSORBOARD_OUTPUT_DIR")

if not tblog_dir:
    print("Error: TENSORBOARD_OUTPUT_DIR env var not set.", file=sys.stderr)
    sys.exit(1)

print("Running benchmark...")
fake_data = np.array([101.2, 100.5, 102.1, 99.8, 101.5])

# Write the raw data to TensorBoard.
try:
    writer = tf.summary.create_file_writer(tblog_dir)
    with writer.as_default():
        for i, value in enumerate(fake_data):
            # The tag "wall_time" MUST match the "name" in your MetricSpec.
            tf.summary.scalar("wall_time", value, step=i)

    writer.flush()
    writer.close()
    print("Successfully wrote metrics.")

except Exception as e:
    print(f"Error writing TensorBoard logs: {e}", file=sys.stderr)
    sys.exit(1)
```

## Step 4: Add repository to runner registry

Finally, the infrastructure needs to know what hardware is available to your repository. This is defined in a central "allowlist" file: [gha_runners.json](https://github.com/google-ml-infra/actions/blob/main/benchmarking/config/gha_runners.json).

If your workflow fails with an error like `Error: No runner pool defined for repository 'my-org/my-repo'`, please file a bug to have your repository and its available runners added to this file.

## Step 5: Testing

When adding or modifying benchmarks, it is often useful to verify the configuration in the GitHub environment before merging. To avoid creating unnecessary PRs (which can clutter history) just to trigger a run, we recommend using a push-based workflow for testing.

**Configure a test trigger**: Ensure your workflow file (from Step 1) is configured to trigger on push to your specific testing branch.

```yaml
on:
  push:
    branches:
      - "benchmarking" # or your feature branch name
```

**Push to the testing branch**: Commit your changes to this branch and push to GitHub.

```bash
git checkout -b benchmarking
git add .
git commit -m "Test new benchmark config"
git push origin benchmarking
```

**Verify the run**: Navigate to the Actions tab in your GitHub repository. You should see the workflow triggered by your push.

1. Monitor the "Run benchmark" jobs to ensure the workload executes successfully.
2. Check the "Parse TensorBoard logs" step output to confirm your metrics were found and parsed correctly.
3. Verify the generated "Benchmark Result" artifacts.
