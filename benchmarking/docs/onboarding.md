# Onboarding Guide: ML Benchmark

This guide provides the steps to add a project's benchmarks to the ML Benchmark platform. ML Benchmark is GitHub-native and makes use of GitHub Actions to administer benchmarks.

The system is designed to execute any GitHub Action as a workload (e.g., standard Python scripts, Bazel targets, or custom user-defined actions), provided it adheres to the metric reporting contract.

The system follows two simple contracts:

1. **Input**: A benchmark registry (e.g., benchmark_registry.pbtxt) defining the workload action and its inputs, along with hardware requirements and other metadata.
2. **Output**: Metric data written via TensorBoard to the standard output directory.

Our infrastructure handles the following:

- Provisioning the correct GitHub Actions runners.
- Converting defined benchmarks and hardware requirements into GitHub Actions jobs.
- Securely executing the specified workload action.
- TensorBoard log parsing and statistic computation.
- Static threshold analysis.

## Step 1: Create a workflow file

First, in your own repository, create a new workflow file in `.github/workflows/` for running benchmarks. 

```yaml
name: Run presubmit benchmarks

on:
  pull_request:
    paths:
      - 'benchmarking/**'

permissions:
  contents: read

jobs:
  run_benchmarks:
    uses: google-ml-infra/actions/.github/workflows/run_benchmarks.yml@<commit | branch | tag>
    with:
      registry_file: "benchmarking/my_registry.pbtxt"
      workflow_type: "PRESUBMIT"
      ml_actions_ref: <commit | branch | tag>
```

### Required permissions

`permissions: contents: read` permission is required in the caller workflow. The reusable workflow's access token inherits permissions from the caller, so the caller must explicitly grant the read rights needed for actions/checkout to succeed.

### ml_actions_ref

You must specify the `ml_actions_ref` input, otherwise it will default to `main`.

This value tells the reusable workflow which version (branch, tag, or SHA) of the google-ml-infra/actions repository to check out for its internal scripts.

For production, use the same stable tag or SHA that's used to pin the reusable workflow file version (e.g. "v1.5.0" for "google-ml-infra/actions/.github/workflows/run_benchmarks.yml@v1.5.0").

### Workflow granularity

We recommend creating a dedicated workflow file for each distinct [workflow_type](https://github.com/google-ml-infra/actions/blob/main/benchmarking/proto/benchmark_registry.proto#L112) 
you plan to support (PRESUBMIT, NIGHTLY, PERIODIC, etc.) to better control scheduling, triggers, and resource allocation.


## Step 2: Create benchmark registry

Next, create a benchmark registry file (.pbtxt) based on the [benchmark_registry.proto](https://github.com/google-ml-infra/actions/blob/main/benchmarking/proto/benchmark_registry.proto) schema. This file defines what code to run and how to run it.

### Defining workloads

The registry uses a flexible schema where you define a workload by specifying an Action to execute and a map of inputs to pass to that action.

We provide standard workload executors for Python and Bazel, but you are fully empowered to define your own custom action in your repository and reference it here.

#### Standard Executors

Each of the standard executors can be referenced either via a remote reference or a local reference. The local reference is recommended since the platform ships the standard executors together with the workflow.

**Python**

- Local reference: `./ml_actions/benchmarking/actions/workload_executors/python`
- Remote reference: `google-ml-infra/actions/benchmarking/actions/workload_executors/python@<ref>`

**Bazel**

- Local reference: `./ml_actions/benchmarking/actions/workload_executors/bazel`
- Remote reference: `google-ml-infra/actions/benchmarking/actions/workload_executors/bazel@<ref>`

#### Workload inputs

You can define base inputs in the `workload` block of the benchmark registry and hardware-specific overrides or extensions in the `hardware_configs` block using `workload_action_inputs`.

For our standard executors, we support "extension" inputs (suffixed with _hw) that allow you to append flags instead of overwriting them.

Note: The infrastructure performs a simple dictionary merge on inputs. If a key in `hardware_configs.workload_action_inputs` matches a key in the base `workload.action_inputs`,  the hardware value will completely overwrite the base value. For best practice, if you are creating your own custom action and want to support appending values (like adding extra flags instead of replacing them), you should define distinct input keys in your action definition (e.g., flags and flags_hw). Your action's script is then responsible for concatenating them.

### Defining metrics

A key part of the registry is defining metrics. You must specify the `metrics.name` field, which must exactly match the tag name used in the TensorBoard logs generated by your benchmark script (covered in the next step). 
Within the metrics block, you specify the statistics (stats) to be calculated (e.g., MEAN, P99) and can optionally configure static threshold analysis using the comparison block.

### Example 1: Bazel workload

This example uses the standard Bazel executor.

Available inputs for Bazel:

- `target` (Required)
- `bazel_run_flags` / `bazel_run_flags_hw` (Passed to bazel run, e.g., compilation options)
- `runtime_flags` / `runtime_flags_hw` (Passed to the binary after --)


```proto
benchmarks {
  name: "my_bazel_benchmark"
  description: "Runs a simple Bazel target."
  owner: "my-team"

  workload {
    # Point to the standard Bazel executor
    action: "./ml_actions/benchmarking/actions/workload_executors/bazel"

    action_inputs { key: "target" value: "//my_project:my_benchmark_binary" }

    # Base runtime flags
    action_inputs { key: "runtime_flags" value: "--model_name=resnet" }
  }

  hardware_configs {
    hardware_category: CPU_X86
    topology { num_hosts: 1, num_devices_per_host: 1 }
    workflow_type: [PRESUBMIT]

    # Hardware-specific build flags
    workload_action_inputs { key: "bazel_run_flags_hw" value: "--config=linux_cpu_opt" }
    
    # Hardware-specific runtime flags
    workload_action_inputs { key: "runtime_flags_hw" value: "--precision=fp32" }
  }

  update_frequency_policy: QUARTERLY

  metrics {
    # REQUIRED: Must match the TensorBoard tag name (e.g., 'wall_time' in the log)
    name: "wall_time"
    unit: "ms"

    stats {
      stat: MEAN
      comparison: {
        # Configures static threshold analysis against a baseline
        baseline { value: 100.0 } 
        threshold { value: 0.1 }
        improvement_direction: LESS 
      }
    }

    stats {
      stat: P99 
    }
  }
}
```

### Example 2: Python workload

This example uses the standard Python executor. It defines base dependencies (`test`) and appends hardware-specific dependencies (`cuda`) and flags (`--use_gpu`) for the GPU config.

Available inputs for Python:

- `python_version` (Required)
- `script_path` (Required)
- `project_path` (Default: ".")
- `extras` / `extras_hw` (Comma-separated extras)
- `runtime_flags` / `runtime_flags_hw` (Passed to the script)

```proto
benchmarks {
  name: "my_python_benchmark"
  description: "Runs a Python script."
  owner: "my-team"

  workload {
    # Point to the standard Python executor using the local path
    action: "./ml_actions/benchmarking/actions/workload_executors/python"

    # Base inputs
    action_inputs { key: "script_path" value: "benchmarking/scripts/run_pallas.py" }
    action_inputs { key: "python_version" value: "3.11" }
    action_inputs { key: "project_path" value: "." }

    # Base extras (e.g. pip install .[test])
    action_inputs { key: "extras" value: "test" }

    # Base flags
    action_inputs { key: "runtime_flags" value: "--model_name=my_kernel" }
  }
  
  hardware_configs {
    hardware_category: GPU_L4
    topology { num_hosts: 1, num_devices_per_host: 1 }
    workflow_type: [PRESUBMIT]
    
    # Hardware extensions (merged into the inputs above)
    # Appends 'cuda' -> pip install .[test,cuda]
    workload_action_inputs { key: "extras_hw" value: "cuda" }

    # Appends flag -> python script.py --model_name=my_kernel --use_gpu
    workload_action_inputs { key: "runtime_flags_hw" value: "--use_gpu" }
  }

  update_frequency_policy: QUARTERLY

  metrics {
    # REQUIRED: Must match the TensorBoard tag name
    name: "throughput"
    unit: "samples/sec"
    
    stats {
      stat: MEAN
      comparison: {
        baseline { value: 5000.0 }
        threshold { value: 0.05 }
        improvement_direction: GREATER
      }
    }

    stats {
      stat: MIN
    }
  }
}
```

## Step 3: Log metrics via TensorBoard

Your benchmark script will need to log metrics via TensorBoard to integrate with the infrastructure. The reusable workflow provides a standard environment variable, `TENSORBOARD_OUTPUT_DIR`, which points to the directory where TensorBoard must write event files.

This environment variable is automatically injected into the execution environment of any workload action (standard or custom). If you define your own workload executor, you simply need to ensure your script reads this variable.

### Supported Libraries

The infrastructure parses both V1 (Scalar) and V2 (Tensor) event formats. You can use any standard writer library:

- [TensorFlow](https://pypi.org/project/tensorflow/)(`tf.summary`): Writes V2 Tensor events.
- [tensorboardX](https://pypi.org/project/tensorboardX/): Writes V1 Scalar events (Ideal for PyTorch users).
- [TensorBoard](https://pypi.org/project/tensorboard/): Writes V1 Scalar events (Lightweight, no TensorFlow dependency).

### Option 1: Using TensorFlow (V2)

Standard approach if your workload already uses TensorFlow.

```python
import tensorflow as tf
import os
import sys
import numpy as np

# Get the output directory from the infrastructure.
tblog_dir = os.environ.get("TENSORBOARD_OUTPUT_DIR")

if not tblog_dir:
    print("Error: TENSORBOARD_OUTPUT_DIR env var not set.", file=sys.stderr)
    sys.exit(1)

fake_data = np.array([101.2, 100.5, 102.1, 99.8, 101.5])

try:
    # Uses the V2 'tensor' bucket
    writer = tf.summary.create_file_writer(tblog_dir)
    with writer.as_default():
        for i, value in enumerate(fake_data):
            # The tag "wall_time" MUST match the "name" in your MetricSpec.
            tf.summary.scalar("wall_time", value, step=i)

    writer.flush()
    writer.close()
    print("Successfully wrote metrics.")

except Exception as e:
    print(f"Error writing TensorBoard logs: {e}", file=sys.stderr)
    sys.exit(1)
```

### Option 2: Using tensorboardX (V1)

Recommended for PyTorch users or lightweight scripts avoiding a heavy TensorFlow installation.

```python
import os
import sys
from tensorboardX import SummaryWriter

tblog_dir = os.environ.get("TENSORBOARD_OUTPUT_DIR")

if not tblog_dir:
    print("Error: TENSORBOARD_OUTPUT_DIR env var not set.", file=sys.stderr)
    sys.exit(1)

fake_data = [101.2, 100.5, 102.1, 99.8, 101.5]

try:
    # Uses the V1 'simple_value' bucket
    writer = SummaryWriter(log_dir=tblog_dir)
    
    for i, value in enumerate(fake_data):
        # The tag "wall_time" MUST match the "name" in your MetricSpec.
        writer.add_scalar("wall_time", value, global_step=i)

    writer.close()
    print("Successfully wrote metrics.")

except Exception as e:
    print(f"Error writing TensorBoard logs: {e}", file=sys.stderr)
    sys.exit(1)
```

### Option 3: Using TensorBoard (V1)

Recommended if you want zero heavy dependencies (no tensorflow and no torch). This uses the low-level protobufs directly.

```python
import os
import sys
import time
from tensorboard.compat.proto import event_pb2, summary_pb2
from tensorboard.summary.writer.event_file_writer import EventFileWriter

tblog_dir = os.environ.get("TENSORBOARD_OUTPUT_DIR")
if not tblog_dir:
    sys.exit("Error: TENSORBOARD_OUTPUT_DIR env var not set.")

fake_data = [101.2, 100.5, 102.1, 99.8, 101.5]

try:
    # Manually writes V1 'simple_value' events
    writer = EventFileWriter(tblog_dir)

    for i, value in enumerate(fake_data):
        event = event_pb2.Event(
            step=i,
            wall_time=time.time(),
            summary=summary_pb2.Summary(
              # The tag "wall_time" MUST match the "name" in your MetricSpec.
              value=[summary_pb2.Summary.Value(tag="wall_time", simple_value=value)]
            )
        )
        writer.add_event(event)

    writer.close()
    print("Successfully wrote metrics.")

except Exception as e:
    print(f"Error writing TensorBoard logs: {e}", file=sys.stderr)
    sys.exit(1)
```

## Step 4: Add repository to runner registry

Finally, the infrastructure needs to know what hardware is available to your repository. This is defined in a central "allowlist" file: [gha_runners.json](https://github.com/google-ml-infra/actions/blob/main/benchmarking/config/gha_runners.json).

If your workflow fails with an error like `Error: No runner pool defined for repository 'my-org/my-repo'`, please file a bug to have your repository and its available runners added to this file.

## Step 5: Testing

When adding or modifying benchmarks, it is often useful to verify the configuration in the GitHub environment before merging. To avoid creating unnecessary PRs (which can clutter history) just to trigger a run, we recommend using a push-based workflow for testing.

**Configure a test trigger**: Ensure your workflow file (from Step 1) is configured to trigger on push to your specific testing branch.

```yaml
on:
  push:
    branches:
      - "benchmarking" # or your feature branch name
```

**Push to the testing branch**: Commit your changes to this branch and push to GitHub.

```bash
git checkout -b benchmarking
git add .
git commit -m "Test new benchmark config"
git push origin benchmarking
```

**Verify the run**: Navigate to the Actions tab in your GitHub repository. You should see the workflow triggered by your push.

1. Monitor the "Run benchmark" jobs to ensure the workload executes successfully.
2. Check the "Parse TensorBoard logs" step output to confirm your metrics were found and parsed correctly.
3. Verify the generated "Benchmark Result" artifacts.
